"""
ë°ì´í„° ì¦ê°•ìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í¬ ë¶„ì„ ëª¨ë“ˆ

ì „ëµ:
- ë‰´ìŠ¤ íƒ€ì´í‹€ Ã— 5ê°ì •(ë¶„ë…¸ ì œì™¸) Ã— ëŒ“ê¸€ìˆ˜ ê· ë“± ìƒì„±
- ê¸°ì¡´ sentiment_comments ë°ì´í„°ëŠ” ê°ì • ë¶„í¬ ì°¸ê³ ìš©
"""

import os
import json
import random
import logging
from collections import Counter
from typing import Dict, List, Optional

import pandas as pd
from datasets import Dataset
from huggingface_hub import hf_hub_download

logger = logging.getLogger(__name__)

SENTIMENT_LABEL_MAP = {
    "ë¶„ë…¸": 0, "ìŠ¬í””": 1, "ë¶ˆì•ˆ": 2, "ìƒì²˜": 3, "ë‹¹í™©": 4, "ê¸°ì¨": 5,
}
LABEL_ID_TO_NAME = {v: k for k, v in SENTIMENT_LABEL_MAP.items()}

# ğŸ”¥ ìƒì„± ëŒ€ìƒ ê°ì • (ë¶„ë…¸ ì œì™¸)
GENERATE_EMOTIONS = ["ìŠ¬í””", "ë¶ˆì•ˆ", "ìƒì²˜", "ë‹¹í™©", "ê¸°ì¨"]


class AugmentationDataset:
    """ì¦ê°• ëŒ€ìƒ ë°ì´í„°ì…‹ ê´€ë¦¬"""

    def __init__(self, config):
        self.config = config
        self.base_dataset: Optional[Dataset] = None
        self.label_counts: Dict[str, int] = {}
        self._news_titles: Optional[List[str]] = None

    # =====================================================
    # Base sentiment dataset (reference only)
    # =====================================================
    def load_base_data(self) -> Dataset:
        """ê¸°ì¡´ ê°ì • ë°ì´í„° ë¡œë“œ (ë¶„í¬ ì°¸ê³ ìš©, ë¶„ë…¸ ì œì™¸)"""
        print("[DATA] Loading base sentiment parquet from HuggingFace Hub")

        parquet_path = hf_hub_download(
            repo_id="MindCastSogang/MindCastTrainSet",
            filename="sentiment_comments/train-00000-of-00001.parquet",
            repo_type="dataset",
        )

        df = pd.read_parquet(parquet_path)

        label_col = None
        for c in ["label", "sentiment", "emotion"]:
            if c in df.columns:
                label_col = c
                break

        if label_col is None:
            raise ValueError(f"ê°ì • ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {df.columns}")

        df["label"] = df[label_col].map(SENTIMENT_LABEL_MAP)
        df = df.dropna(subset=["label"])
        df["label"] = df["label"].astype(int)

        self.base_dataset = Dataset.from_pandas(df, preserve_index=False)

        print(f"[DATA] Loaded {len(self.base_dataset)} samples (reference only)")
        return self.base_dataset

    def get_label_distribution(self) -> Dict[str, int]:
        if self.base_dataset is None:
            raise RuntimeError("load_base_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")

        counter = Counter(self.base_dataset["label"])
        self.label_counts = {
            LABEL_ID_TO_NAME[k]: v
            for k, v in counter.items()
            if k in LABEL_ID_TO_NAME
        }
        return self.label_counts

    # =====================================================
    # News title loader (JSON direct parsing)
    # =====================================================
    def get_news_titles(self) -> List[str]:
        """
        ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        - hf_hub_download + json.load ì‚¬ìš©
        - title ì¤‘ë³µ ì œê±°
        - timestamp schema ë¬¸ì œ ì™„ì „ íšŒí”¼
        """
        if self._news_titles is not None:
            return self._news_titles

        if not self.config.news_repo_files:
            logger.warning("[DATA] news_repo_filesê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ ì‚¬ìš©")
            self._news_titles = _get_sample_news_titles()
            return self._news_titles

        print(f"[DATA] Loading news titles from {self.config.news_dataset_id}")

        titles = set()

        for rel_path in self.config.news_repo_files:
            local_path = hf_hub_download(
                repo_id=self.config.news_dataset_id,
                filename=rel_path,
                repo_type="dataset",
            )

            with open(local_path, "r", encoding="utf-8") as f:
                obj = json.load(f)

            for day in obj.get("data", []):
                for post in day.get("posts", []):
                    title = post.get("title")
                    if title:
                        titles.add(title.strip())

        self._news_titles = sorted(titles)
        print(f"[DATA] Loaded {len(self._news_titles)} unique news titles")
        return self._news_titles

    # =====================================================
    # Generation plan
    # =====================================================
    def get_full_label_distribution(self) -> Dict[str, int]:
        """
        ë¶„ë…¸ í¬í•¨ ì „ì²´ ê°ì • ë¶„í¬ (anchor ê¸°ì¤€ìš©)
        """
        parquet_path = hf_hub_download(
            repo_id="MindCastSogang/MindCastTrainSet",
            filename="sentiment_comments/train-00000-of-00001.parquet",
            repo_type="dataset",
        )

        df = pd.read_parquet(parquet_path)

        label_col = None
        for c in ["label", "sentiment", "emotion"]:
            if c in df.columns:
                label_col = c
                break

        if label_col is None:
            raise ValueError("ê°ì • ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        return dict(Counter(df[label_col]))

    def compute_final_generation_plan_anchor_to_anger(self) -> Dict:
        """
        ë¶„ë…¸ë¥¼ ê¸°ì¤€(anchor)ìœ¼ë¡œ
        ë‹¤ë¥¸ ê°ì •ë“¤ì„ ë¶„ë…¸ ê°œìˆ˜ê¹Œì§€ ë§ì¶”ëŠ”
        ìµœì¢… ì‹¤í–‰ìš© ìƒì„± ê³„íš (ì •ìˆ˜í™”)
        """

        full_dist = self.get_full_label_distribution()

        if "ë¶„ë…¸" not in full_dist:
            raise RuntimeError("ë¶„ë…¸ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        anger_count = full_dist["ë¶„ë…¸"]

        news_titles = self.get_news_titles()
        num_titles = len(news_titles)

        final_plan = {
            "anchor_emotion": "ë¶„ë…¸",
            "anchor_count": anger_count,
            "num_titles": num_titles,
            "emotions": {},
            "total_to_generate": 0,
        }

        for emo in GENERATE_EMOTIONS:
            current = full_dist.get(emo, 0)
            need = max(0, anger_count - current)

            # íƒ€ì´í‹€ë‹¹ ê¸°ë³¸ ìƒì„± ê°œìˆ˜ (floor)
            per_title = need // num_titles
            remainder = need % num_titles

            final_plan["emotions"][emo] = {
                "current": current,
                "target": anger_count,
                "need_to_generate": need,
                "per_title": per_title,
                "extra_titles": remainder,  # ì•ì—ì„œë¶€í„° +1 ì¤„ íƒ€ì´í‹€ ìˆ˜
            }

            final_plan["total_to_generate"] += need

        return final_plan
    
    def print_final_generation_plan(self, plan: Dict) -> None:
        print("\n" + "=" * 60)
        print("ğŸ”¥ FINAL GENERATION PLAN (ANGER-ANCHORED)")
        print("=" * 60)

        print(f"Anchor emotion : {plan['anchor_emotion']}")
        print(f"Anchor count  : {plan['anchor_count']}")
        print(f"News titles   : {plan['num_titles']}")
        print("-" * 60)

        for emo, info in plan["emotions"].items():
            print(f"[{emo}]")
            print(f"  current           : {info['current']}")
            print(f"  target            : {info['target']}")
            print(f"  need_to_generate  : {info['need_to_generate']}")
            print(f"  per_title         : {info['per_title']}")
            print(f"  extra_titles (+1) : {info['extra_titles']}")
            print("-" * 60)

        print(f"TOTAL GENERATED COMMENTS: {plan['total_to_generate']}")
        print("=" * 60)

    def compute_generation_plan(self) -> Dict:
        news_titles = self.get_news_titles()
        num_titles = len(news_titles)

        emotions = GENERATE_EMOTIONS
        min_c = self.config.min_comments_per_title
        max_c = self.config.max_comments_per_title

        plan = {
            "num_titles": num_titles,
            "emotions": emotions,
            "per_emotion_min": num_titles * min_c,
            "per_emotion_max": num_titles * max_c,
            "total_min": num_titles * len(emotions) * min_c,
            "total_max": num_titles * len(emotions) * max_c,
        }

        print("\n[PLAN] ìƒì„± ê³„íš")
        print(f"  íƒ€ì´í‹€ ìˆ˜: {num_titles}")
        print(f"  ê°ì •: {', '.join(emotions)}")
        print(f"  ì´ ìƒì„±ëŸ‰: {plan['total_min']:,} ~ {plan['total_max']:,}")

        return plan

    # =====================================================
    # Few-shot examples
    # =====================================================
    def get_fewshot_examples(self, n: int = 5) -> Dict[str, List[str]]:
        """ê°ì •ë³„ few-shot ì˜ˆì‹œ ëŒ“ê¸€ì„ ë² ì´ìŠ¤ ë°ì´í„°ì—ì„œ nê°œì”© ìƒ˜í”Œë§

        Returns:
            {"ìŠ¬í””": ["ëŒ“ê¸€1", "ëŒ“ê¸€2", ...], "ë¶ˆì•ˆ": [...], ...}
        """
        parquet_path = hf_hub_download(
            repo_id=self.config.base_dataset_id,
            filename=self.config.base_dataset_parquet,
            repo_type="dataset",
        )
        df = pd.read_parquet(parquet_path)

        examples = {}
        for emotion in self.config.generate_emotions:
            subset = df[df["label"] == emotion]["text"].dropna().tolist()
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê±´ ì œì™¸
            subset = [t.strip() for t in subset if 10 <= len(t.strip()) <= 80]
            if len(subset) > n:
                sampled = random.sample(subset, n)
            else:
                sampled = subset
            examples[emotion] = sampled

        total = sum(len(v) for v in examples.values())
        print(f"[DATA] Few-shot ì˜ˆì‹œ ë¡œë“œ: {total}ê°œ ({n}ê°œ Ã— {len(examples)}ê°ì •)")
        return examples

    # =====================================================
    # Utils
    # =====================================================
    def print_reference_distribution(self) -> None:
        if not self.label_counts:
            self.get_label_distribution()

        total = sum(self.label_counts.values())
        print("\nê¸°ì¡´ ë°ì´í„° ë¼ë²¨ ë¶„í¬ (ì°¸ê³ )")
        for emo in ["ë¶„ë…¸","ìŠ¬í””", "ë¶ˆì•ˆ", "ìƒì²˜", "ë‹¹í™©", "ê¸°ì¨"]:
            cnt = self.label_counts.get(emo, 0)
            ratio = cnt / total * 100 if total > 0 else 0
            print(f"  {emo}: {cnt} ({ratio:.1f}%)")


def _get_sample_news_titles() -> List[str]:
    return [
        "ì½”ë¡œë‚˜19 í™•ì§„ì ê¸‰ì¦, ì •ë¶€ ê¸´ê¸‰ ëŒ€ì±… ë°œí‘œ",
        "ë¶€ë™ì‚° ê°€ê²© í­ë“±, ì²­ë…„ì¸µ ë‚´ ì§‘ ë§ˆë ¨ ë” ì–´ë ¤ì›Œì ¸",
        "ì·¨ì—…ë‚œ ì‹¬í™”, ì²­ë…„ ì‹¤ì—…ë¥  ì—­ëŒ€ ìµœê³ ì¹˜ ê¸°ë¡",
        "ê¸°í›„ë³€í™” ì‹¬ê°, ì—­ëŒ€ê¸‰ í­ìš°ë¡œ í”¼í•´ ì†ì¶œ",
    ]
